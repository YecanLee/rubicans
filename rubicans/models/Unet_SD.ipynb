{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_diffusion_model import SpatialTransformer, TimeStepEmbSequantial,Upsample, Downsample\n",
    "\n",
    "import numpy as numpy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int, \n",
    "                 d_t_embedding: int,\n",
    "                 out_channels=None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        channels: input dimensions,\n",
    "        d_t_embedding: time_embedding,\n",
    "        out_channels: out_channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.d_t_embedding = d_t_embedding\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if self.out_channels is None:\n",
    "            self.out_channels = channels\n",
    "        \n",
    "        self.in_layers = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, num_channels=channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )  \n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_t_embedding, out_channels)\n",
    "        )\n",
    "\n",
    "        self.out_layers = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, nun_channels=channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        if out_channels==channels:\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            self.shortcut = nn.Conv2d(channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "        \n",
    "        def forward(self, x:torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.in_layers(x)\n",
    "            t_emb = self.emb_layers(t_emb)\n",
    "            x = x + t_emb[:, :, None, None]\n",
    "            x = self.out_layers(x)\n",
    "            shortcut = self.shortcut(x)\n",
    "            output = shortcut + x\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: torch.Tensor,\n",
    "                 out_channels: torch.Tensor,\n",
    "                 channels: torch.Tensor,\n",
    "                 num_blocks: int,\n",
    "                 atten_level: List[int],\n",
    "                 channel_multiplier: List[int],\n",
    "                 num_heads: int,\n",
    "                 tf_layers: int,\n",
    "                 emb_dim: int,\n",
    "                 clip_dim: int,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        levels = len(self.channel_multiplier)\n",
    "        \n",
    "        d_time_embedding = 4 * self.channels\n",
    "\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(channels, d_time_embedding),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_time_embedding, channels)\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.ModuleList([])\n",
    "        self.downsample.append(TimeStepEmbSequantial(\n",
    "            nn.Conv2d(in_channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        ))\n",
    "\n",
    "        input_block_channels = [channels]\n",
    "        channels_list = [channels * m for m in channel_multiplier]\n",
    "        \n",
    "        # This would be the downsample part of the UNet, the resnet does not have a different output channel\n",
    "        for i in range(levels):\n",
    "            for _ in range(num_blocks):\n",
    "                layers =  [ResBlock(channels, d_t_embedding=d_time_embedding, out_channels=channels_list[i])]\n",
    "                channels = channels_list[i]\n",
    "            \n",
    "            if i in atten_level:\n",
    "                layers.append(SpatialTransformer(channels, num_heads, tf_layers, emb_dim))\n",
    "            \n",
    "            self.downsample.append(TimeStepEmbSequantial(*layers))\n",
    "            input_block_channels.append(channels)\n",
    "        \n",
    "            # Downsample at all levels except the last one\n",
    "            if i != levels-1:\n",
    "               self.downsample.append(TimeStepEmbSequantial(Downsample(channels)))\n",
    "               input_block_channels.append(channels)\n",
    "        \n",
    "        # Build the bottleneck block\n",
    "        self.middle_block = TimeStepEmbSequantial(\n",
    "            ResBlock(channels, d_time_embedding),\n",
    "            SpatialTransformer(channels, num_heads, tf_layers, emb_dim, clip_dim),\n",
    "            ResBlock(channels, d_time_embedding)\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.ModuleList([])\n",
    "        \n",
    "        # num_blocks has one more in the upsample part\n",
    "        for i in reversed(range(levels)):\n",
    "            for j in range(num_blocks + 1):\n",
    "                layers = [ResBlock(channels + input_block_channels.pop(), d_t_embedding=d_time_embedding, out_channels=channels_list[i])]\n",
    "                channels = channels_list[i]\n",
    "\n",
    "                if i in atten_level:\n",
    "                    layers.append(SpatialTransformer(channels, num_heads, tf_layers, emb_dim))\n",
    "                \n",
    "                if i!=0 and j==num_blocks:\n",
    "                    layers.append(Upsample(channels))\n",
    "\n",
    "                self.upsample.append(TimeStepEmbSequantial(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            torch.GroupNorm(num_groups=32, num_channels=channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def time_step_embedding(self, time_step:torch.Tensor, max_period:int=10000):\n",
    "        half = self.channels//2 #half of the channels would be sin and other half would be cosine\n",
    "        frequencies = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32))/half\n",
    "        args = time_step[:, None].float() * frequencies[None]\n",
    "        output = torch.cat(torch.cos(args), torch.sin(args), dim=-1)\n",
    "        return output\n",
    "    \n",
    "    def forward(self,x: torch.Tensor, time_steps: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        calculate the time step embedding and add it for our transformer model\n",
    "        \"\"\"\n",
    "        x_input_block = []\n",
    "        \n",
    "        embedding = self.time_step_embedding(x)\n",
    "        embedding = self.time_emb(x)\n",
    "        \n",
    "        # Encoder part of the UNet\n",
    "        for module in self.downsample:\n",
    "            x = module(x, embedding, cond)\n",
    "            x_input_block.append(x)\n",
    "        \n",
    "        # BottleNeck part of the UNet\n",
    "        x = self.middle_block(x, embedding, cond)\n",
    "\n",
    "        # Decoder part of the UNet\n",
    "        for module in self.upsample:\n",
    "            x = module(x, embedding, cond)\n",
    "            x_input_block.append(x)\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeStepEmbSequantial(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 x:torch.Tensor,\n",
    "                 t_emb: torch.Tensor,\n",
    "                 cond = None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x: torch.Tensor, t_emb: torch.Tensor, cond: None) -> torch.Tensor:\n",
    "        for layer in self:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                x = layer(x, t_emb)\n",
    "            elif isinstance(layer, SpatialTransformer):\n",
    "                x = layer(x, cond)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        This is the part inside every upsample block, so it's the upsample block itself\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This upsample class follows the original UNet structure, the model will interpolate first, then downsample once\n",
    "        \"\"\"\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        output = self.conv(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels:int,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv = nn.Conv2d(channels, channels, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This downsample class follows the original UNet structure, the model will downsample without interpolation\n",
    "        \"\"\"\n",
    "        output = self.conv(x)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
