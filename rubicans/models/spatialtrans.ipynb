{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from stable_diffusion_model import BaSicTransformerBlock, CrossAttentionBlock, LinearCrossAttentionBlock, FeedFoward, Normal_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int, \n",
    "                 heads: int, \n",
    "                 block_num: int, \n",
    "                 model_dim: int, \n",
    "                 clip_dim:int,\n",
    "                 linear:List[None]\n",
    "                 ): \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.heads = heads\n",
    "        self.block_num = block_num\n",
    "        self.model_dim = model_dim\n",
    "        self.clip_dim = clip_dim\n",
    "        self.linear = linear\n",
    "\n",
    "        self.norm = torch.nn.GroupNorm(32, channels, eps=1e-6, affine=True)\n",
    "        self.proj_in = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BaSicTransformerBlock(channels, heads, channels//heads, cond_dim = clip_dim) for _ in range(block_num)]\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "\n",
    "    def forward(self, x:torch.Tensor, clip_dim:torch.Tensor, linear:List[None], fast_connect: bool = False) -> torch.Tensor:\n",
    "        b, c, h, w = x.shape\n",
    "        if fast_connect:\n",
    "            x_in = nn.Identity(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.proj_in(x)\n",
    "        x = rearrange(x, 'b c h w -> b h*w c')\n",
    "        # x = x.permute(0, 2, 3, 1).view(b, h*w, c)\n",
    "        for blocks in self.transformer_blocks:\n",
    "            x = self.transformer_blocks(x, )\n",
    "        x = rearrange(x, 'b h*w c -> b c h w')\n",
    "        x = self.proj_out(x)\n",
    "        x_out = x_in + x\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two options here, one would be the linear focus attention, another one would be the normal cross attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaSicTransformerBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emd_dim: torch.Tensor,\n",
    "                 num_heads: torch.Tensor,\n",
    "                 head_dim: torch.Tensor,\n",
    "                 cond_dim: torch.Tensor,\n",
    "                 linear = List[None],\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        The model_dim would be the input channels of the ViT model, thus no explicit model_dim param is needed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emd_dim = emd_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.linear = linear\n",
    "\n",
    "        if self.linear[0] is not None:\n",
    "            self.attention1 = LinearCrossAttentionBlock(emd_dim, emd_dim, num_heads, head_dim)\n",
    "        else:\n",
    "            self.attention1 = CrossAttentionBlock(emd_dim, emd_dim, num_heads, head_dim)\n",
    "        self.norm1 = nn.LayerNorm(emd_dim)\n",
    "\n",
    "        if self.linear[1] is not None:\n",
    "            self.attention2 = LinearCrossAttentionBlock(emd_dim, cond_dim, num_heads, head_dim)\n",
    "        else:\n",
    "            self.attention2 = CrossAttentionBlock(emd_dim, cond_dim, num_heads, head_dim)\n",
    "        self.norm2 = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        self.ffn = FeedFoward(emd_dim)\n",
    "        self.norm3 = nn.LayerNorm(emd_dim)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, cond:torch.Tensor):\n",
    "        x = self.norm1(self.attention1(x)) + x\n",
    "        x = self.norm2(self.attention2(x, cond = cond)) + x\n",
    "        x = self.norm3(self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_attention = Normal_Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 emd_dim: int, \n",
    "                 cond_dim: int, \n",
    "                 num_heads: int, \n",
    "                 head_dim: int, \n",
    "                 use_spe_attn: bool = False,\n",
    "                 inplace:bool = True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.emd_dim = emd_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.inplace = inplace\n",
    "\n",
    "        self.scale_factor = head_dim ** 0.5\n",
    "        model_dim = self.head_dim ** 0.5\n",
    "        self.to_q = nn.Linear(self.emb_dim, model_dim, bias=False)\n",
    "        self.to_k = nn.Linear(self.emb_dim, model_dim, bias=False)\n",
    "        self.to_v = nn.Linear(self.emb_dim, model_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Linear(model_dim,self.emb_dim)\n",
    "\n",
    "        try:\n",
    "            from stable_diffusion_model import EfficientAttention\n",
    "            self.efficient_attention = EfficientAttention()\n",
    "        except ImportError:\n",
    "            self.efficient_attention = None\n",
    "\n",
    "    def forward(self, x:torch.Tensor, cond:Optional[torch.Tensor]=None):\n",
    "        has_cond = cond is not None\n",
    "        if has_cond is None:\n",
    "            cond = x\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(v)\n",
    "        \n",
    "        if self.efficient_attention is not None and self.use_spe_attn == True and has_cond is not None and self.head_dim >= 128:\n",
    "            return self.efficient_attention(q, k, v)\n",
    "        else:\n",
    "            return self.normal_attention(q, k, v)\n",
    "        \n",
    "    def normal_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        split the Q, K, V into multiple heads and calculate the attention\n",
    "        \"\"\"\n",
    "        q = rearrange(q, 'b l (d h) -> b l h d')\n",
    "        k = rearrange(k, 'b l (d h) -> b l h d')\n",
    "        v = rearrange(v, 'b l (d h) -> b l h d')\n",
    "    \n",
    "        attn = torch.einsum('b i h d, b j h d -> b h i j', q, k) * self.scale_factor\n",
    "\n",
    "        if self.inplace:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
