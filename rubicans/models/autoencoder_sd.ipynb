{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn  as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "from stable_diffusion_model import Encoder, Decoder, GaussianDistribution, ResNet, Downsample, AttnBlock, Upsample\n",
    "from helper import Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels:int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        output = self.conv(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsaple(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.Conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.pad(x, (0, 1, 0, 1), mode='constant', value=0)\n",
    "        output = self.Conv(x) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int,\n",
    "                 out_channels: int):\n",
    "        super().__init_()\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.short_cut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.short_cut = nn.Identity()\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.norm1(x)\n",
    "        x = Swish(x)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.norm2(x)\n",
    "        x = Swish(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x_short = self.short_cut(x)\n",
    "        output = x_short + x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int,\n",
    "                 channels_multiplier: List[int],\n",
    "                 num_blocks: int,\n",
    "                 in_channels: int,\n",
    "                 z_channels: int,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        channels: the channels in the first CNN layer,\n",
    "        channels_multiplier: multiply the channels in the following layer,\n",
    "        num_blocks: the number of resnet blocks in the model,\n",
    "        in_channels: input image channels,\n",
    "        z_channels: hidden space channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_resolution = len(channels_multiplier)\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        channels_list = [m*channels for m in [1]+channels_multiplier]\n",
    "\n",
    "        self.down = nn.ModuleList()\n",
    "        for i in range(n_resolution):\n",
    "            # each layer of the model has several resnet block as well as downsample blocks\n",
    "            resnet_block = []\n",
    "            for j in range(num_blocks):\n",
    "                resnet_block.append(ResNet(channels, channels_list[i + 1]))\n",
    "                # update the channels of the next resnet block to the output layer of this block\n",
    "                channels = channels_list[i + 1]\n",
    "            \n",
    "            down = nn.Module()\n",
    "            down.block = resnet_block\n",
    "\n",
    "            if i != n_resolution-1:\n",
    "                down.downsample = Downsample(channels)\n",
    "            else:\n",
    "                down.downsample = nn.Identity()\n",
    "\n",
    "            self.down.append(down)\n",
    "        \n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResNet(channels, channels)\n",
    "        self.mid.attn_1 = AttnBlock(channels)\n",
    "        self.mid.block_2 = ResNet(channels, channels)\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(num_groups=32, num_channels=channels)\n",
    "        self.conv_out = nn.Conv2d(channels, 2 * z_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_in(x)\n",
    "        for i in self.down:\n",
    "            for block in self.down.block:\n",
    "                x = block(x)\n",
    "            x = self.down.downsample(x)\n",
    "\n",
    "            x = self.mid.block_1(x)\n",
    "            x = self.mid.attn_1(x)\n",
    "            x = self.mid.block_2(x)\n",
    "\n",
    "            x = self.norm_out(x)\n",
    "            x = Swish(x)\n",
    "            x = self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int,\n",
    "                 channels_multiplier: List[int],\n",
    "                 num_blocks: int,\n",
    "                 out_channels: int,\n",
    "                 z_channels: int,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        out_channels: the output channel of the generated image\n",
    "        other features are the same as encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        num_resolutions = len(channels_multiplier)\n",
    "\n",
    "        channels_list = [m * channels for m in channels_multiplier]\n",
    "        \n",
    "        channels = channels_list[-1]\n",
    "\n",
    "        self.conv_in = nn.Conv2d(z_channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResNet(channels, channels)\n",
    "        self.mid.attn_1 = AttnBlock(channels)\n",
    "        self.mid.block_2 = ResNet(channels, channels)\n",
    "\n",
    "        self.up = nn.ModuleList()\n",
    "\n",
    "        for i in range(reversed(num_resolutions)):\n",
    "            resnet_blocks = nn.ModuleList()\n",
    "            for _ in range(num_blocks+1):\n",
    "                resnet_blocks.append(ResNet(channels, channels_list[i]))\n",
    "                channels = channels_list[i]\n",
    "\n",
    "            up = nn.Module()\n",
    "            up.block = resnet_blocks\n",
    "            # Upsample at the end of each block except the first one\n",
    "            if i != 0:\n",
    "                up.upsample = Upsample(channels)\n",
    "            else:\n",
    "                up.upsample = nn.Identity()\n",
    "            \n",
    "            # This is just for the consistency with the pretrained model\n",
    "            self.up.insert(0, up)\n",
    "            \n",
    "        self.norm_out = nn.GroupNorm(num_groups=32, num_channels=channels)\n",
    "        self.conv_out = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, z:torch.Tensor) -> torch.Tensor:\n",
    "        h = self.conv_in(z)\n",
    "        h = self.mid.block_1(h)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h)\n",
    "\n",
    "        for up in reversed(self.up):\n",
    "            for block in up.block:\n",
    "                h = block(h)\n",
    "            h = up.upsample(h)\n",
    "        h = self.norm_out(h)\n",
    "        h = Swish(h)\n",
    "        image = self.conv_out(h)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDistribution(nn.Module):\n",
    "    def __init__(self, \n",
    "                 parameters: torch.Tensor,\n",
    "                 upper_bound: int = 20,\n",
    "                 lower_bound: int = -30\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        The parameters has shape: (batch_size, z_channels *2, z_height, z_width)\n",
    "        This is the output shape of the Encoder, mean and variance parameters are needed for reparameterization tricks\n",
    "        \"\"\"\n",
    "        super().__init()\n",
    "        self.mean,log_var = torch.chunk(parameters, 2, dim=1)\n",
    "        self.log_var = torch.clamp(log_var, lower_bound, upper_bound)\n",
    "        self.std = torch.exp(0.5 * self.log_var)\n",
    "\n",
    "    def sample(self):\n",
    "        output = self.mean + self.std * torch.randn_like(self.std)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: torch.Tensor\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.norm = nn.GroupNorm(num_groups=32, num_channels=channels)\n",
    "        self.to_q = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.to_k = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.to_v = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # Channelwise CNN\n",
    "        self.proj_out = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = channels ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        \n",
    "        q = q.view(b, c, h*w)\n",
    "        k = k.view(b, c, h*w)\n",
    "        v = v.view(b, c, h*w)\n",
    "\n",
    "        attn = torch.einsum('bci, bcj -> bij', q, k)\n",
    "        attn = attn* self.scale\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        output = torch.einsum('bej, bfj -> bfe', attn, v)\n",
    "\n",
    "        output = output.view(b, c, h, w)\n",
    "        output = self.proj_out(x)\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder: 'Encoder', \n",
    "                 decoder: 'Decoder', \n",
    "                 emb_channels: int, \n",
    "                 z_channels: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # The convolution layer to project embedding space to the reparameterization space\n",
    "        self.quant_conv = nn.Conv2d(2*z_channels, 2*emb_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # The convolution layer to projrct the embedding from reparameterization space to embedding space\n",
    "        self.post_quant_conv = nn.Conv2d(emb_channels, z_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> 'GaussianDistribution':\n",
    "        z = self.encoder(x)\n",
    "        moments = self.quant_conv(x)\n",
    "        output = GaussianDistribution(moments)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.post_quant_conv(z)\n",
    "        output = self.decoder(x)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
