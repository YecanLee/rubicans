{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example to understand the shape of following attention visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 1, 1])\n",
      "torch.Size([128, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dwc = nn.Conv2d(3, 3, kernel_size = 1, stride=1, padding=0)\n",
    "weights = dwc.weight\n",
    "print(weights.shape)\n",
    "\n",
    "second_dwc = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=7, stride=1, padding=1)\n",
    "second_weights = second_dwc.weight\n",
    "print(second_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the convolution kernel would be: out_channels, in_channels, kernel_size, kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "Attention_1 = torch.nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
    "dummy_input = torch.rand(32, 128, 512)\n",
    "q = dummy_input\n",
    "k = dummy_input\n",
    "v = dummy_input\n",
    "with torch.no_grad():\n",
    "    attn_output, attn_output_weights = Attention_1(q, k, v)\n",
    "\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the attention would be (batch_size, input_length, model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Visualization(nn.Module):\n",
    "    def __init__(self, \n",
    "                 qk: torch.Tensor,\n",
    "                 attn: torch.Tensor,\n",
    "                 kernel: Optional[None],\n",
    "                 name: Optional[str]=None):\n",
    "        \"\"\"\n",
    "        The kernel here would be the linear attention dwc kernel\n",
    "        Not the ReLU kernel used in the code\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (qk is None and attn is not None) or qk is not None and attn is None\n",
    "        if qk is not None:\n",
    "            self.attn = self.qk[0] * self.qk[1].transpose(-2, -1)\n",
    "            self.attn = self.attn / self.attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        if len(self.attn.shape) == 4:\n",
    "            self.attn = self.attn[:, 0, :, :]\n",
    "        self.kernel = kernel\n",
    "        self.name = name\n",
    "\n",
    "        os.makedirs('./visualize', exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def set_flag(path: str, flag):\n",
    "        with open(os.path.join(path, 'flag.txt'), mode='w') as f:\n",
    "            f.write(str(flag))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_flag(path):\n",
    "        if not os.path.exists(os.path.join(path, 'flag.txt')):\n",
    "            flag=0\n",
    "        else:\n",
    "            with open(os.path.join(path, 'flag.txt'), mode='r') as f:\n",
    "                flag = int(f.readlines()[-1])\n",
    "        return flag\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_image(image:Union[torch.Tensor, np.array], attn:Union[torch.Tensor, np.array], color=None, alpha: int=0.3):\n",
    "        \"\"\"\n",
    "        If you want to call this function, please call by using self.mask_image()...\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_attn_matrix(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        this function would only get the first head of the attn matrix, since we would \n",
    "        use a loop for all the attn heads we have in the next function,\n",
    "        check the github issue about the visualization, this attention matrix has a shape (1 ,N, N)\n",
    "        \"\"\"\n",
    "        attn_matrix = self.attn[0, :, :].clone()\n",
    "        if self.kernel is not None:\n",
    "            kernel = self.kernel[0, 0, :, :].clone()\n",
    "            # the attn_matrix.shape[0] would be height*width = N, \n",
    "            # the following function will get the height and width of the input image\n",
    "            a = int(attn_matrix.shape[0] ** 0.5)\n",
    "            n = int(kernel.shape[0] - 1 /2)\n",
    "            conv_masks = torch.zeros(size=(attn_matrix[0], kernel.shape[1] + n * (a+1) *2))\n",
    "            for i in range(attn_matrix.shape[0]):\n",
    "                for j in range(kernel.shape[0]):\n",
    "                    conv_masks[i, i + j*a: i + j*a + kernel.shape[1] + n * (a+1) *2]\n",
    "            conv_mask = conv_mask[:, n * (a+1):n * (a+1) + attn_matrix.shape[1]]\n",
    "            attn_matrix = attn_matrix + conv_mask\n",
    "            # return the absolute value of the attention matrix in case the value of the attention matrix would be minus\n",
    "            attn_matrix = torch.abs(attn_matrix)\n",
    "\n",
    "        # normalization of the attention matrix value for visualization\n",
    "        attn_matrix = attn_matrix/attn_matrix.sum(dim=-1, keepdim=True)\n",
    "        # exponential enlarge the attention matrix value so the visualization result would be better\n",
    "        attn_matrix = attn_matrix * (attn_matrix.shape[0]/196) * 10\n",
    "        attn_matrix[attn_matrix>1] = 1\n",
    "        return attn_matrix\n",
    "    \n",
    "    def get_all_attn(self, max_num:Optional[int]=None) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        This function tries to get all the attention matrix,  the first step would be using the get_attn_matrix to get a single attention matrix\n",
    "        the attn_remain function works for ensuring attention matrix has a square shape\n",
    "        \"\"\"\n",
    "        attn = self.get_attn_matrix()\n",
    "        shape_remain = attn.shape[1] - int(int(attn.shape[1] ** 0.5)**2)\n",
    "        n = attn.shape[0]\n",
    "        m = attn.shape[1] - shape_remain\n",
    "        shape = [int(m ** 0.5), int(m ** 0.5)]\n",
    "        if max_num is not None:\n",
    "            import math\n",
    "            # math.ceil() function returns the smallest integral value greater than the number.\n",
    "            # for example math.ceil(1.85) = 2\n",
    "            sep = math.ceil(n/max_num)\n",
    "            n = n // sep\n",
    "        all_attn = []\n",
    "        for i in range(n):\n",
    "            if max_num is None:\n",
    "                temp = attn[i, shape_remain:]\n",
    "            else:\n",
    "                temp = attn[i * sep, shape_remain:]\n",
    "            temp_np = temp.reshape(shape[0], shape[1]).cpu().numpy()\n",
    "            all_attn.append(temp_np)\n",
    "        return all_attn\n",
    "\n",
    "    def visualize_all_attentions(self, max_num:Optional[None], image:Optional[torch.Tensor]=None, **kwargs) -> torch.Tensor:\n",
    "        path = './visualize/' + self.name + '_all'\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        all_attn = self.get_all_attn(max_num=None, **kwargs)\n",
    "        flag = self.get_flag(path=path)\n",
    "        count = flag\n",
    "        if not os.path.exists(path + '/' + self.name + '_' + str(count)):\n",
    "            os.mkdir(path + '/' + self.name + '_' + str(count))\n",
    "        if image is None:\n",
    "            norm = matplotlib.colors.Normalize(vmin=0, vmax=1)\n",
    "            for i in tqdm(range(len(all_attn))):\n",
    "                # plt.matshow: display an array as a matrix\n",
    "                plt.matshow(all_attn[i], cmap='Blues', norm=norm)\n",
    "                plt.colorbar()\n",
    "                plt.title('Attention Mask')\n",
    "                plt.savefig(path + '/' + self.name + '_' + str(count) + '/' + str(i) + '.png', dpi=600)\n",
    "                plt.close()\n",
    "        else:\n",
    "            image = np.array(Image.open(image))\n",
    "            for i in tqdm(range(len(all_attn))):\n",
    "                result = self.mask_image(image, all_attn[i])\n",
    "                result.save(path + '/' + self.name + '_' + str(count) + '/' + str(i) + '.png')\n",
    "            if count == 0:\n",
    "                n = all_attn[0].shape[0]*all_attn[0].shape[1]\n",
    "                # sep = 1 this is not needed\n",
    "                if max_num is not None:\n",
    "                    import math\n",
    "                    sep = math.ceil(n/max_num)\n",
    "                    n = n//sep\n",
    "                    if not os.path.exists(path + './query'):\n",
    "                        os.mkdir(path + './query')\n",
    "                    for i in range(n):\n",
    "                        attn = np.zeros(shape=(all_attn.shape[0], all_attn.shape[1], dtype=float))\n",
    "                        attn[(i * sep) // all_attn[0].shape[1], (i * sep) % all_attn[0].shape[1]] = 1.0\n",
    "                        result = self.mask_image(image, attn, alpha=-1, color=[255., 33., 33.])\n",
    "                        result.save(path + '/query/' + str(i) + '.png')\n",
    "        self.set_flag(path=path,flag=flag+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
