{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "from typing import Optional, Union, List, Dict, Tuple, Set, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n",
    "    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n",
    "    Transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_size, patch_size = 224, 16\n",
    "        num_channels, hidden_size = 3, 768\n",
    "\n",
    "        image_size = (image_size, image_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "        if not interpolate_pos_encoding:\n",
    "            if height != self.image_size[0] or width != self.image_size[1]:\n",
    "                raise ValueError(\n",
    "                    f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "                )\n",
    "        # the shape would be [batch_size, hidden_size, height//patch_size, width//patch_size] before the flatten operation, after flatten we have [batch_size, hidden_size, num_patches]\n",
    "        # then after the transpose operation, we have [batch_size, num_patches, hidden_size]\n",
    "        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_embedding = ViTPatchEmbeddings()\n",
    "pixel_values = torch.rand(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    embeddings = test_embedding(pixel_values)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfAttention(nn.Module):\n",
    "    def __init__(self, ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attention_heads = 12\n",
    "        self.attention_head_size = int(768 / 12)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(768, self.all_head_size, bias=True)\n",
    "        self.key = nn.Linear(768, self.all_head_size, bias=True)\n",
    "        self.value = nn.Linear(768, self.all_head_size, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0., inplace=False)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_image = torch.randn(32, 196, 768)\n",
    "vitattention = ViTSelfAttention()\n",
    "with torch.no_grad():\n",
    "    output = vitattention(test_image)\n",
    "    print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_image2 = torch.randn(32, 196, 768)\n",
    "vitoutput = ViTSelfOutput()\n",
    "with torch.no_grad():\n",
    "    output = vitoutput(test_image2, test_image2)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAttention(nn.Module):\n",
    "    def __init__(self,) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTSelfAttention()\n",
    "        self.output = ViTSelfOutput()\n",
    "\n",
    "    def forward(self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False,):\n",
    "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_image_3 = torch.randn(32, 196, 768)\n",
    "vitattention_whole = ViTAttention()\n",
    "with torch.no_grad():\n",
    "    output = vitattention_whole(test_image_3)\n",
    "    print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # self.cls_token = nn.Parameter(torch.randn(1, 1, 768))\n",
    "        # self.mask_token = nn.Parameter(torch.zeros(1, 1, 768)) if use_mask_token else None\n",
    "        self.patch_embeddings = ViTPatchEmbeddings()\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches, 768))\n",
    "        self.dropout = nn.Dropout(0., inplace=False)\n",
    "\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove the cls token, since we are only using the ViT Encoder to encode the image\n",
    "        # If no interpolation is needed, return the position embeddings as they are\n",
    "        num_patches = embeddings.shape[1]\n",
    "        num_positions = self.position_embeddings.shape[1]\n",
    "        if num_patches == num_positions and height == width:\n",
    "            return self.position_embeddings\n",
    "        # the following two lines of code are commented out because we are not using the class token\n",
    "        # class_pos_embed = self.position_embeddings[:, 0]\n",
    "        # patch_pos_embed = self.position_embeddings[:, 1:]\n",
    "        dim = embeddings.shape[-1]\n",
    "        h0 = height // 16\n",
    "        w0 = width // 16\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        h0, w0 = h0 + 0.1, w0 + 0.1\n",
    "        # the int function in the following function is to make sure h and w are integers, so the weird number problem after we add some special tokens will be solved\n",
    "        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed,\n",
    "            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        assert int(h0) == patch_pos_embed.shape[-2] and int(w0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return patch_pos_embed\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "\n",
    "        # the parameters should be kept since they are calling the forward function of the ViTPatchEmbeddings class\n",
    "        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "        \n",
    "        # comment out the CLS token since we are not using it\n",
    "        # add the [CLS] token to the embedded patch tokens\n",
    "        # cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "\n",
    "        # change the embedding length if it is longer than the default length, use the interpolation function defined earlier\n",
    "        if interpolate_pos_encoding:\n",
    "            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n",
    "        else:\n",
    "            embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_image = torch.randn(1, 3, 224, 224)\n",
    "embedding = ViTPatchEmbeddings()\n",
    "with torch.no_grad():\n",
    "    output = embedding(test_image)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "test_image2 = torch.randn(1, 3, 224, 224)\n",
    "embedding = ViTEmbeddings()\n",
    "with torch.no_grad():\n",
    "    output = embedding(test_image2)\n",
    "    print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
